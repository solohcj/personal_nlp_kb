{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bdaaf9-c4ac-49ba-9d90-221430ed8938",
   "metadata": {},
   "source": [
    "---\n",
    "## This notebook will contain the functions which will be useful for Triplet Loss Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51af78c-2dbc-4d4d-8901-9547cf4fed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\solom\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from numpy.linalg import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4640f10f-c457-491b-9f4a-482bbdba06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f2198d-668a-4669-b753-161b09db0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e0af33-74d5-4c5a-abd2-6376bc59ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to GPU if not it will be processed with CPU\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116efd40-45c8-4825-99ea-0faffd205362",
   "metadata": {},
   "source": [
    "---\n",
    "### Useful Functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063eae36-579a-4ac7-80e4-5736fe46d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(anchor_array, comparison_array):\n",
    "    \"\"\"\n",
    "    Does consine similarity comparison via matrix operations.\n",
    "    \n",
    "    Faster than pair by pair comparison using sklearn's cosine_similarity function as this calculates anchor to multiple comparisons via single matrix operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.dot(comparison_array, anchor_array)/(norm(comparison_array, axis=1)*norm(anchor_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bef6054-9bf8-40d3-85a3-d07218eeab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_outputs, attention_mask):\n",
    "    \"\"\"\n",
    "    See hugging face documentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    token_embeddings = model_outputs[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f276ec14-2785-436b-9edd-a978802d7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_hard_neg_mining(checkpoint_input_df, model, tokenizer):\n",
    "    updated_training_dataset = pd.DataFrame(columns=checkpoint_input_df.columns)\n",
    "    \n",
    "    encoded_df = checkpoint_input_df.copy()\n",
    "    encoded_df.reset_index(inplace=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_counter = 0\n",
    "        \n",
    "        for i in range(encoded_df.shape[0]):\n",
    "            if ((i%200)==0) & (i!=0):\n",
    "                if (i%50000)==0:\n",
    "                    print (\"Embedding and comparing completed till \" + str(i))\n",
    "                    \n",
    "                batch_df = encoded_df.iloc[start_counter:i, :]\n",
    "                \n",
    "                anchor_batch = batch_df['anchor'].tolist()\n",
    "                pos_batch = batch_df['positive'].tolist()\n",
    "                neg_batch = batch_df['negative'].tolist()\n",
    "                \n",
    "                encoded_anchor = tokenizer(anchor_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_anchor_batch_output = model(**encoded_anchor)\n",
    "                batch_anchor_sentence_embeddings = mean_pooling(model_anchor_batch_output, encoded_anchor['attention_mask'])\n",
    "                \n",
    "                encoded_pos = tokenizer(pos_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_pos_batch_output = model(**encoded_pos)\n",
    "                batch_pos_sentence_embeddings = mean_pooling(model_pos_batch_output, encoded_pos['attention_mask'])\n",
    "                \n",
    "                encoded_neg = tokenizer(neg_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_neg_batch_output = model(**encoded_neg)\n",
    "                batch_neg_sentence_embeddings = mean_pooling(model_neg_batch_output, encoded_anchor['attention_mask'])\n",
    "                \n",
    "                batch_df['anchor_embeddings'] = batch_anchor_sentence_embeddings.tolist()\n",
    "                batch_df['pos_embeddings'] = batch_pos_sentence_embeddings.tolist()\n",
    "                batch_df['neg_embeddings'] = batch_neg_sentence_embeddings.tolist()\n",
    "                \n",
    "                for index in batch_df.index:\n",
    "                    anchor = batch_df.loc[index, 'anchor_embeddings']\n",
    "                    positive = batch_df.loc[index, 'pos_embeddings']\n",
    "                    negative = batch_df.loc[index, 'neg_embeddings']\n",
    "                    \n",
    "                    distance_pos = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(positive).reshape(1,-1))[0][0]\n",
    "                    distance_neg = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(negative).reshape(1,-1))[0][0]\n",
    "                    \n",
    "                    # Hard Negative: d(A,P) > d(A,N)\n",
    "                    # Distance of the positive should be smaller/closer to the anchor than the negative\n",
    "                    if distance_pos > distance_neg:\n",
    "                        updated_training_dataset = updated_training_dataset.append(batch_df.loc[index, ['anchor', 'positive', 'negative']])\n",
    "                \n",
    "                start_counter = i\n",
    "                \n",
    "            if i==(encoded_df.shape[0]-1):\n",
    "                print (\"Embedding and comparing completed till \" + str(i))\n",
    "                batch_df = encoded_df.iloc[start_counter:, :]\n",
    "\n",
    "                anchor_batch = batch_df['anchor'].tolist()\n",
    "                pos_batch = batch_df['positive'].tolist()\n",
    "                neg_batch = batch_df['negative'].tolist()\n",
    "\n",
    "                encoded_anchor = tokenizer(anchor_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_anchor_batch_output = model(**encoded_anchor)\n",
    "                batch_anchor_sentence_embeddings = mean_pooling(model_anchor_batch_output, encoded_anchor['attention_mask'])\n",
    "\n",
    "                encoded_pos = tokenizer(pos_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_pos_batch_output = model(**encoded_pos)\n",
    "                batch_pos_sentence_embeddings = mean_pooling(model_pos_batch_output, encoded_pos['attention_mask'])\n",
    "\n",
    "                encoded_neg = tokenizer(neg_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_neg_batch_output = model(**encoded_neg)\n",
    "                batch_neg_sentence_embeddings = mean_pooling(model_neg_batch_output, encoded_anchor['attention_mask'])\n",
    "\n",
    "                batch_df['anchor_embeddings'] = batch_anchor_sentence_embeddings.tolist()\n",
    "                batch_df['pos_embeddings'] = batch_pos_sentence_embeddings.tolist()\n",
    "                batch_df['neg_embeddings'] = batch_neg_sentence_embeddings.tolist()\n",
    "\n",
    "                for index in batch_df.index:\n",
    "                    anchor = batch_df.loc[index, 'anchor_embeddings']\n",
    "                    positive = batch_df.loc[index, 'pos_embeddings']\n",
    "                    negative = batch_df.loc[index, 'neg_embeddings']\n",
    "\n",
    "                    distance_pos = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(positive).reshape(1,-1))[0][0]\n",
    "                    distance_neg = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(negative).reshape(1,-1))[0][0]\n",
    "\n",
    "                    # Hard Negative: d(A,P) > d(A,N)\n",
    "                    # Distance of the positive should be smaller/closer to the anchor than the negative\n",
    "                    if distance_pos > distance_neg:\n",
    "                        updated_training_dataset = updated_training_dataset.append(batch_df.loc[index, ['anchor', 'positive', 'negative']])\n",
    "                        \n",
    "    return updated_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16f0d3c3-4561-445a-8cc0-b7e52edf6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_semi_hard_neg_mining(checkpoint_input_df, model, tokenizer, margin):\n",
    "    updated_training_dataset = pd.DataFrame(columns=checkpoint_input_df.columns)\n",
    "    \n",
    "    encoded_df = checkpoint_input_df.copy()\n",
    "    encoded_df.reset_index(inplace=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_counter = 0\n",
    "        \n",
    "        for i in range(encoded_df.shape[0]):\n",
    "            if ((i%200)==0) & (i!=0):\n",
    "                if (i%50000)==0:\n",
    "                    print (\"Embedding and comparing completed till \" + str(i))\n",
    "                    \n",
    "                batch_df = encoded_df.iloc[start_counter:i, :]\n",
    "                \n",
    "                anchor_batch = batch_df['anchor'].tolist()\n",
    "                pos_batch = batch_df['positive'].tolist()\n",
    "                neg_batch = batch_df['negative'].tolist()\n",
    "                \n",
    "                encoded_anchor = tokenizer(anchor_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_anchor_batch_output = model(**encoded_anchor)\n",
    "                batch_anchor_sentence_embeddings = mean_pooling(model_anchor_batch_output, encoded_anchor['attention_mask'])\n",
    "                \n",
    "                encoded_pos = tokenizer(pos_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_pos_batch_output = model(**encoded_pos)\n",
    "                batch_pos_sentence_embeddings = mean_pooling(model_pos_batch_output, encoded_pos['attention_mask'])\n",
    "                \n",
    "                encoded_neg = tokenizer(neg_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_neg_batch_output = model(**encoded_neg)\n",
    "                batch_neg_sentence_embeddings = mean_pooling(model_neg_batch_output, encoded_anchor['attention_mask'])\n",
    "                \n",
    "                batch_df['anchor_embeddings'] = batch_anchor_sentence_embeddings.tolist()\n",
    "                batch_df['pos_embeddings'] = batch_pos_sentence_embeddings.tolist()\n",
    "                batch_df['neg_embeddings'] = batch_neg_sentence_embeddings.tolist()\n",
    "                \n",
    "                for index in batch_df.index:\n",
    "                    anchor = batch_df.loc[index, 'anchor_embeddings']\n",
    "                    positive = batch_df.loc[index, 'pos_embeddings']\n",
    "                    negative = batch_df.loc[index, 'neg_embeddings']\n",
    "                    \n",
    "                    distance_pos = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(positive).reshape(1,-1))[0][0]\n",
    "                    distance_neg = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(negative).reshape(1,-1))[0][0]\n",
    "                    \n",
    "                    # Semi Hard Negative: d(A,P) < d(A,N) < d(A,P)+margin\n",
    "                    # Distance of positive is closer than negative to anchor but negtive not yet of margin away from positive distance\n",
    "                    if (distance_pos<distance_neg) & (distance_neg<(distance_pos+margin)):\n",
    "                        updated_training_dataset = updated_training_dataset.append(batch_df.loc[index, ['anchor', 'positive', 'negative']])\n",
    "                \n",
    "                start_counter = i\n",
    "                \n",
    "            if i==(encoded_df.shape[0]-1):\n",
    "                print (\"Embedding and comparing completed till \" + str(i))\n",
    "                batch_df = encoded_df.iloc[start_counter:, :]\n",
    "\n",
    "                anchor_batch = batch_df['anchor'].tolist()\n",
    "                pos_batch = batch_df['positive'].tolist()\n",
    "                neg_batch = batch_df['negative'].tolist()\n",
    "\n",
    "                encoded_anchor = tokenizer(anchor_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_anchor_batch_output = model(**encoded_anchor)\n",
    "                batch_anchor_sentence_embeddings = mean_pooling(model_anchor_batch_output, encoded_anchor['attention_mask'])\n",
    "\n",
    "                encoded_pos = tokenizer(pos_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_pos_batch_output = model(**encoded_pos)\n",
    "                batch_pos_sentence_embeddings = mean_pooling(model_pos_batch_output, encoded_pos['attention_mask'])\n",
    "\n",
    "                encoded_neg = tokenizer(neg_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_neg_batch_output = model(**encoded_neg)\n",
    "                batch_neg_sentence_embeddings = mean_pooling(model_neg_batch_output, encoded_anchor['attention_mask'])\n",
    "\n",
    "                batch_df['anchor_embeddings'] = batch_anchor_sentence_embeddings.tolist()\n",
    "                batch_df['pos_embeddings'] = batch_pos_sentence_embeddings.tolist()\n",
    "                batch_df['neg_embeddings'] = batch_neg_sentence_embeddings.tolist()\n",
    "\n",
    "                for index in batch_df.index:\n",
    "                    anchor = batch_df.loc[index, 'anchor_embeddings']\n",
    "                    positive = batch_df.loc[index, 'pos_embeddings']\n",
    "                    negative = batch_df.loc[index, 'neg_embeddings']\n",
    "\n",
    "                    distance_pos = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(positive).reshape(1,-1))[0][0]\n",
    "                    distance_neg = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(negative).reshape(1,-1))[0][0]\n",
    "\n",
    "                    # Semi Hard Negative: d(A,P) < d(A,N) < d(A,P)+margin\n",
    "                    # Distance of positive is closer than negative to anchor but negtive not yet of margin away from positive distance\n",
    "                    if (distance_pos<distance_neg) & (distance_neg<(distance_pos+margin)):\n",
    "                        updated_training_dataset = updated_training_dataset.append(batch_df.loc[index, ['anchor', 'positive', 'negative']])\n",
    "                        \n",
    "    return updated_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2f166c6-e537-4eb0-afee-0ce0dee1b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_hardest_neg_mining(checkpoint_input_df, model, tokenizer, size):\n",
    "    updated_training_dataset = pd.DataFrame(columns=checkpoint_input_df.columns+['score'])\n",
    "    \n",
    "    encoded_df = checkpoint_input_df.copy()\n",
    "    encoded_df.reset_index(inplace=True)\n",
    "    \n",
    "    encoded_df['score'] = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_counter = 0\n",
    "        \n",
    "        for i in range(encoded_df.shape[0]):\n",
    "            if ((i%200)==0) & (i!=0):\n",
    "                if (i%50000)==0:\n",
    "                    print (\"Embedding and comparing completed till \" + str(i))\n",
    "                    \n",
    "                batch_df = encoded_df.iloc[start_counter:i, :]\n",
    "                \n",
    "                anchor_batch = batch_df['anchor'].tolist()\n",
    "                pos_batch = batch_df['positive'].tolist()\n",
    "                neg_batch = batch_df['negative'].tolist()\n",
    "                \n",
    "                encoded_anchor = tokenizer(anchor_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_anchor_batch_output = model(**encoded_anchor)\n",
    "                batch_anchor_sentence_embeddings = mean_pooling(model_anchor_batch_output, encoded_anchor['attention_mask'])\n",
    "                \n",
    "                encoded_pos = tokenizer(pos_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_pos_batch_output = model(**encoded_pos)\n",
    "                batch_pos_sentence_embeddings = mean_pooling(model_pos_batch_output, encoded_pos['attention_mask'])\n",
    "                \n",
    "                encoded_neg = tokenizer(neg_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_neg_batch_output = model(**encoded_neg)\n",
    "                batch_neg_sentence_embeddings = mean_pooling(model_neg_batch_output, encoded_anchor['attention_mask'])\n",
    "                \n",
    "                batch_df['anchor_embeddings'] = batch_anchor_sentence_embeddings.tolist()\n",
    "                batch_df['pos_embeddings'] = batch_pos_sentence_embeddings.tolist()\n",
    "                batch_df['neg_embeddings'] = batch_neg_sentence_embeddings.tolist()\n",
    "                \n",
    "                for index in batch_df.index:\n",
    "                    anchor = batch_df.loc[index, 'anchor_embeddings']\n",
    "                    positive = batch_df.loc[index, 'pos_embeddings']\n",
    "                    negative = batch_df.loc[index, 'neg_embeddings']\n",
    "                    \n",
    "                    distance_pos = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(positive).reshape(1,-1))[0][0]\n",
    "                    distance_neg = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(negative).reshape(1,-1))[0][0]\n",
    "                    \n",
    "                    batch_df.loc[index, ['score']] = distance_neg - distance_pos\n",
    "                    \n",
    "                    # Hard Negative: d(A,P) > d(A,N)\n",
    "                    # Distance of the positive should be smaller/closer to the anchor than the negative\n",
    "                    if distance_pos > distance_neg:\n",
    "                        updated_training_dataset = updated_training_dataset.append(batch_df.loc[index, ['anchor', 'positive', 'negative', 'score']])\n",
    "                \n",
    "                start_counter = i\n",
    "                \n",
    "            if i==(encoded_df.shape[0]-1):\n",
    "                print (\"Embedding and comparing completed till \" + str(i))\n",
    "                batch_df = encoded_df.iloc[start_counter:, :]\n",
    "\n",
    "                anchor_batch = batch_df['anchor'].tolist()\n",
    "                pos_batch = batch_df['positive'].tolist()\n",
    "                neg_batch = batch_df['negative'].tolist()\n",
    "\n",
    "                encoded_anchor = tokenizer(anchor_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_anchor_batch_output = model(**encoded_anchor)\n",
    "                batch_anchor_sentence_embeddings = mean_pooling(model_anchor_batch_output, encoded_anchor['attention_mask'])\n",
    "\n",
    "                encoded_pos = tokenizer(pos_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_pos_batch_output = model(**encoded_pos)\n",
    "                batch_pos_sentence_embeddings = mean_pooling(model_pos_batch_output, encoded_pos['attention_mask'])\n",
    "\n",
    "                encoded_neg = tokenizer(neg_batch, max_length=150, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "                model_neg_batch_output = model(**encoded_neg)\n",
    "                batch_neg_sentence_embeddings = mean_pooling(model_neg_batch_output, encoded_anchor['attention_mask'])\n",
    "\n",
    "                batch_df['anchor_embeddings'] = batch_anchor_sentence_embeddings.tolist()\n",
    "                batch_df['pos_embeddings'] = batch_pos_sentence_embeddings.tolist()\n",
    "                batch_df['neg_embeddings'] = batch_neg_sentence_embeddings.tolist()\n",
    "\n",
    "                for index in batch_df.index:\n",
    "                    anchor = batch_df.loc[index, 'anchor_embeddings']\n",
    "                    positive = batch_df.loc[index, 'pos_embeddings']\n",
    "                    negative = batch_df.loc[index, 'neg_embeddings']\n",
    "\n",
    "                    distance_pos = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(positive).reshape(1,-1))[0][0]\n",
    "                    distance_neg = cosine_similarity(np.array(anchor).reshape(1,-1), np.array(negative).reshape(1,-1))[0][0]\n",
    "                    \n",
    "                    batch_df.loc[index, ['score']] = distance_neg - distance_pos\n",
    "                    \n",
    "                    # Hard Negative: d(A,P) > d(A,N)\n",
    "                    # Distance of the positive should be smaller/closer to the anchor than the negative\n",
    "                    if distance_pos > distance_neg:\n",
    "                        updated_training_dataset = updated_training_dataset.append(batch_df.loc[index, ['anchor', 'positive', 'negative', 'score']])\n",
    "    \n",
    "    updated_training_dataset = updated_training_dataset.sort_values('score', ascending=False)\n",
    "    updated_training_dataset = updated_training_dataset[['anchor', 'positive', 'negative']]\n",
    "    \n",
    "    return updated_training_dataset.iloc[:size, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162400b0-0fda-4961-a225-8311d105705b",
   "metadata": {},
   "source": [
    "---\n",
    "### Sample Fine-tuning\n",
    "\n",
    "Will not be splitting the toy dataset as it is just meant to show the process\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a6ef912-e64e-4e29-932a-c37896d11fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../0. Sample Datasets/quora_question_triplet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca0f32eb-b127-41dd-a6e4-848423917ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              anchor  \\\n",
       "0  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "1  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "2  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "3  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "4                     How can I be a good geologist?   \n",
       "\n",
       "                                            positive  \\\n",
       "0  I'm a triple Capricorn (Sun, Moon and ascendan...   \n",
       "1  I'm a triple Capricorn (Sun, Moon and ascendan...   \n",
       "2  I'm a triple Capricorn (Sun, Moon and ascendan...   \n",
       "3  I'm a triple Capricorn (Sun, Moon and ascendan...   \n",
       "4          What should I do to be a great geologist?   \n",
       "\n",
       "                                            negative  \n",
       "0                     How can I be a good geologist?  \n",
       "1        How do I read and find my YouTube comments?  \n",
       "2               What can make Physics easy to learn?  \n",
       "3        What was your first sexual experience like?  \n",
       "4  Astrology: I am a Capricorn Sun Cap moon and c...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_column', None) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b623e051-dbaf-4dae-b43b-79575a8f2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   anchor    21 non-null     object\n",
      " 1   positive  21 non-null     object\n",
      " 2   negative  21 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 632.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3ed86-5768-4fd6-9851-40750dd14445",
   "metadata": {},
   "source": [
    "#### Loading Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3702b22b-2c4a-48e6-8ed1-350387a4e240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"D:\\\\DSAI\\\\\\Pre-Trained Models\\\\all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"D:\\\\DSAI\\\\\\Pre-Trained Models\\\\all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b034a4-1e17-4441-9162-06c668e3e69f",
   "metadata": {},
   "source": [
    "#### Splitting data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7437de69-2ec0-4ffc-9d08-544d1a07bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(df, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be8e7e97-3e7e-4741-875e-3c3b393699f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_anchor = tokenizer(X_val['anchor'].tolist(), max_length=150, padding=True, truncation=True, return_tensors='pt')\n",
    "X_val_positive = tokenizer(X_val['positive'].tolist(), max_length=150, padding=True, truncation=True, return_tensors='pt')\n",
    "X_val_negative = tokenizer(X_val['negative'].tolist(), max_length=150, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61f6b47e-8cd1-47d1-99ea-61cf9df7b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_anchor_seq = torch.tensor(X_val_anchor['input_ids'])\n",
    "val_anchor_mask = torch.tensor(X_val_anchor['attention_mask'])\n",
    "\n",
    "val_positive_seq = torch.tensor(X_val_positive['input_ids'])\n",
    "val_positive_mask = torch.tensor(X_val_positive['attention_mask'])\n",
    "\n",
    "val_negative_seq = torch.tensor(X_val_negative['input_ids'])\n",
    "val_negative_mask = torch.tensor(X_val_negative['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12d5bc23-3e17-482b-856d-987bb1813381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Wrap tensors\n",
    "val_data = TensorDataset(val_anchor_seq, val_anchor_mask, val_positive_seq, val_positive_mask, val_negative_seq, val_negative_mask)\n",
    "\n",
    "# Sampler for sampling the data during validation for training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# Dataloader for val set\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f2f61-11fa-418a-b838-156d18e5481b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "718b0a73-2d51-4efa-bdab-70cf2b0ff993",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc41af0c-a7c3-47cf-9f16-35cbf668956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define optimiser\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1db04c8f-30d7-4221-9f8e-bda10889d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = nn.TripletMarginWithDistanceLoss(distance_function=nn.CosineSimilarity(), margin=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a008d7cb-2d03-4e45-b29e-b4e5658f0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # Empty list to save model predictions\n",
    "    total_preds = []\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update for every 50 batches\n",
    "        if step%50==0 and not step==0:\n",
    "            print ('Batch {:>5,} of {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # Push batch to GPU\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        anchor_id, anchor_mask, positive_id, positive_mask, negative_id, negative_mask = batch\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get model predictions for the current batch\n",
    "        anchor_output = model(anchor_id, anchor_mask)\n",
    "        positive_output = model(positive_id, positive_mask)\n",
    "        negative_output = model(negative_id, negative_mask)\n",
    "        \n",
    "        \"\"\"\n",
    "        nn.CosineSimilarity measures similarity between 2 outputs, the more similar, the bigger the score.\n",
    "        However for triplet loss, the positive cases are supposed to be closer and have a smaller score.\n",
    "        To make things easier, we flipped the negative and positive positions\n",
    "        i.e. loss(anchor, positive, negative) --> loss(anchor, negative, positive)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute loss \n",
    "        loss = loss_fn(mean_pooling(anchor_output, anchor_mask), mean_pooling(negative_output, negative_mask), mean_pooling(positive_output, positive_mask))\n",
    "\n",
    "        # Add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b81e3f51-6cbb-4818-a10d-840adb1f6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print ('\\nEvaluating...')\n",
    "    \n",
    "    # Deactivate dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # Empty list to save model predictions\n",
    "    total_preds = []\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        # Progress update for every 50 batches\n",
    "        if step%50==0 and not step==0:\n",
    "            print ('Batch {:>5,} of {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # Push batch to GPU\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        anchor_id, anchor_mask, positive_id, positive_mask, negative_id, negative_mask = batch\n",
    "\n",
    "        # Deactivate autograd()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Get model predictions for the current batch\n",
    "            anchor_output = model(anchor_id, anchor_mask)\n",
    "            positive_output = model(positive_id, positive_mask)\n",
    "            negative_output = model(negative_id, negative_mask)\n",
    "            \n",
    "            \"\"\"\n",
    "            nn.CosineSimilarity measures similarity between 2 outputs, the more similar, the bigger the score.\n",
    "            However for triplet loss, the positive cases are supposed to be closer and have a smaller score.\n",
    "            To make things easier, we flipped the negative and positive positions\n",
    "            i.e. loss(anchor, positive, negative) --> loss(anchor, negative, positive)\n",
    "            \"\"\"\n",
    "\n",
    "            # Compute loss \n",
    "            loss = loss_fn(mean_pooling(anchor_output, anchor_mask), mean_pooling(negative_output, negative_mask), mean_pooling(positive_output, positive_mask))\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "    # Compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6688df1-3aff-4bed-92b6-e5094d5fcf0c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/ 30\n",
      "Implementing Offline Semi-Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  1\n",
      "Number of training batches:  1\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.59172165\n",
      "Validation Loss: 0.13888678\n",
      "\n",
      "Epoch 2/ 30\n",
      "Implementing Offline Semi-Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  1\n",
      "Number of training batches:  1\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.43918371\n",
      "Validation Loss: 0.14239602\n",
      "\n",
      "Epoch 3/ 30\n",
      "Implementing Offline Semi-Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  1\n",
      "Number of training batches:  1\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.45824814\n",
      "Validation Loss: 0.14624749\n",
      "\n",
      "Epoch 4/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  17\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.29126200\n",
      "Validation Loss: 0.13453224\n",
      "\n",
      "Epoch 5/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  17\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.15550324\n",
      "Validation Loss: 0.11217611\n",
      "\n",
      "Epoch 6/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  17\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.15960991\n",
      "Validation Loss: 0.09253371\n",
      "\n",
      "Epoch 7/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  17\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.10108471\n",
      "Validation Loss: 0.07862175\n",
      "\n",
      "Epoch 8/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  17\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.13340248\n",
      "Validation Loss: 0.06548258\n",
      "\n",
      "Epoch 9/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.14093840\n",
      "Validation Loss: 0.05316880\n",
      "\n",
      "Epoch 10/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.12366185\n",
      "Validation Loss: 0.04662093\n",
      "\n",
      "Epoch 11/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.09280645\n",
      "Validation Loss: 0.04351286\n",
      "\n",
      "Epoch 12/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.07547072\n",
      "Validation Loss: 0.04067866\n",
      "\n",
      "Epoch 13/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.05706239\n",
      "Validation Loss: 0.03793108\n",
      "\n",
      "Epoch 14/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.07063348\n",
      "Validation Loss: 0.03535537\n",
      "\n",
      "Epoch 15/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.05080821\n",
      "Validation Loss: 0.03280455\n",
      "\n",
      "Epoch 16/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.04364374\n",
      "Validation Loss: 0.02944726\n",
      "\n",
      "Epoch 17/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.06558442\n",
      "Validation Loss: 0.02493544\n",
      "\n",
      "Epoch 18/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.03727140\n",
      "Validation Loss: 0.01659443\n",
      "\n",
      "Epoch 19/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.03669420\n",
      "Validation Loss: 0.00914965\n",
      "\n",
      "Epoch 20/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.03541510\n",
      "Validation Loss: 0.00215620\n",
      "\n",
      "Epoch 21/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.03364808\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 22/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.03335377\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 23/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.02795795\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 24/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.03148878\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 25/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.02403930\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 26/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.02612433\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 27/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.02143363\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 28/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.02294166\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 29/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.01904840\n",
      "Validation Loss: 0.00000000\n",
      "\n",
      "Epoch 30/ 30\n",
      "Implementing Offline Hard Negative Mining\n",
      "Embedding and comparing completed till 17\n",
      "Size of selected dataset:  18\n",
      "Number of training batches:  3\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.02137305\n",
      "Validation Loss: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# No of training epochs\n",
    "epochs = 30\n",
    "\n",
    "# Set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    print ('\\nEpoch {:}/ {:}'.format(epoch+1, epochs))\n",
    "    \n",
    "    # Evaluate and curate dataset for training\n",
    "    # For first 3 epochs use semi-hard negative mining, from 6th epoch onwards all hard negative mining\n",
    "    if epoch<=2:\n",
    "        print(\"Implementing Offline Semi-Hard Negative Mining\")\n",
    "        \n",
    "        offline_triplet_selection = offline_semi_hard_neg_mining(X_train, model, tokenizer, margin=1.0)\n",
    "        \n",
    "        if offline_triplet_selection.shape[0]==0: # If the model is already good and unable to find negative examples, use entire train dataset\n",
    "            offline_triplet_selection = X_train\n",
    "    else:\n",
    "        print(\"Implementing Offline Hard Negative Mining\")\n",
    "        \n",
    "        offline_triplet_selection = offline_hard_neg_mining(X_train, model, tokenizer)\n",
    "        \n",
    "        if offline_triplet_selection.shape[0]==0: # If the model is already good and unable to find negative examples, use entire train dataset\n",
    "            offline_triplet_selection = X_train\n",
    "        \n",
    "    print (\"Size of selected dataset: \", str(offline_triplet_selection.shape[0]))\n",
    "    \n",
    "    # Tokenize selected triplets\n",
    "    X_train_anchor = tokenizer(offline_triplet_selection['anchor'].tolist(), max_length=150, padding=True, truncation=True, return_tensors='pt')\n",
    "    X_train_positive = tokenizer(offline_triplet_selection['positive'].tolist(), max_length=150, padding=True, truncation=True, return_tensors='pt')\n",
    "    X_train_negative = tokenizer(offline_triplet_selection['negative'].tolist(), max_length=150, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Convert list to tensors\n",
    "    train_anchor_seq = torch.tensor(X_train_anchor['input_ids'])\n",
    "    train_anchor_mask = torch.tensor(X_train_anchor['attention_mask'])\n",
    "\n",
    "    train_positive_seq = torch.tensor(X_train_positive['input_ids'])\n",
    "    train_positive_mask = torch.tensor(X_train_positive['attention_mask'])\n",
    "\n",
    "    train_negative_seq = torch.tensor(X_train_negative['input_ids'])\n",
    "    train_negative_mask = torch.tensor(X_train_negative['attention_mask'])\n",
    "    \n",
    "    # Curate new tensor dataset\n",
    "    # Wrap tensors\n",
    "    train_data = TensorDataset(train_anchor_seq, train_anchor_mask, train_positive_seq, train_positive_mask, train_negative_seq, train_negative_mask)\n",
    "\n",
    "    # Sampler for sampling the data during training\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "\n",
    "    # Dataloader for train set\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    print (\"Number of training batches: \", str(len(train_dataloader)))\n",
    "    \n",
    "    # Train model\n",
    "    train_loss = train(train_dataloader=train_dataloader)\n",
    "    \n",
    "    # Evaluate model\n",
    "    valid_loss = evaluate()\n",
    "    \n",
    "    # Save the best model\n",
    "    if valid_loss<best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # torch.save(model.state_dict(), 'saved_model_weights/pytorch_siamese_network.pt')\n",
    "        \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print (f\"\\nTraining Loss: {train_loss:.8f}\")\n",
    "    print (f\"Validation Loss: {valid_loss:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dba1a685-bfe1-49ca-9d8f-30ffef744a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a438a80b90>,\n",
       " <matplotlib.lines.Line2D at 0x2a39750bc90>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, 'g', valid_losses, 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
