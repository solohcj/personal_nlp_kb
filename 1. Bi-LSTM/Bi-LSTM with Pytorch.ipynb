{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51af78c-2dbc-4d4d-8901-9547cf4fed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import itertools\n",
    "\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing model architecture\n",
    "from bilstm_model_architecture import BiLSTMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2198d-668a-4669-b753-161b09db0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0af33-74d5-4c5a-abd2-6376bc59ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to GPU if not it will be processed with CPU\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ef912-e64e-4e29-932a-c37896d11fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../0. Sample Datasets/spam_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f32eb-b127-41dd-a6e4-848423917ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', None) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623e051-dbaf-4dae-b43b-79575a8f2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04daaa11-17f3-48b4-9bcd-c3968fe29b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.v1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3f1ec-39df-4a75-b61b-4d9b18b0aa72",
   "metadata": {},
   "source": [
    "## We will clean all data (regardless of test/val/train) with the same process before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5296eac-1f16-45a3-88d2-f1783c45c259",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lem = nltk.WordNetLemmatizer()\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sent = sentence.lower()\n",
    "    \n",
    "    # Removing selected symbols, keeping numbers\n",
    "    sent = re.sub(\"\\(|\\)|\\/|\\-|\\#|\\!|\\?|\\.|\\,|\\\"|\\'\", \"\", sent)\n",
    "    \n",
    "    # Removing emails\n",
    "    sent = re.sub(\"\\S*@\\S*\\s?\", \"\", sent)\n",
    "    \n",
    "    # Removing numbers\n",
    "    sent = re.sub(\"\\d+\", \"\", sent)\n",
    "    \n",
    "    sent = sent.split() # Splitting\n",
    "    \n",
    "    # Lemmatisation and stopword removal\n",
    "    sent = [lem.lemmatize(word) for word in sent if not word in stop_words]\n",
    "    sent = \" \".join(sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3c921-30aa-4ce2-ae33-eda0afff6c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e1359-eebe-45d8-a665-2114e48ce36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30bed9-074d-4c49-9117-873c2e79bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing('Ok lar... Joking wif u oni...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f61e8-303b-43f3-9a56-8ec18797f98c",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6a530-8795-4b78-a1dc-2afc731e7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.v2\n",
    "y = df.v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437de69-2ec0-4ffc-9d08-544d1a07bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7805e-d8f5-4514-bb28-13866dde7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14fa149-ac87-41b3-ba32-6efbef7a3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb2bec-b2e3-4934-b07a-91afd250da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c79b2-0841-435d-b524-ae2c3bd6dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99770347-68ca-4b19-9398-8aefcef36754",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e2e94d-15a5-496c-b50a-2b5e95df1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting indexes for subsequent processing (less confusing to tally)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_val.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_val.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f47b584-b9f8-4b2c-806e-9317d0ff5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff4132-8715-4800-8e1e-6e2ac584efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0e68f-15cd-4a18-937f-b576d11709bd",
   "metadata": {},
   "source": [
    "## Binarising the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ce483-582a-4811-8d92-831ccce515d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y_train_ohe = lb.fit_transform(y_train)\n",
    "y_val_ohe = lb.transform(y_val)\n",
    "y_test_ohe = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6d2e9-be7b-4ff3-9a24-bc28f7215d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da9ed1-774e-47d7-b4ac-84eb92923cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86360a41-f647-4316-aa97-4eb0e08e6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ohe[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d0f7e-94b0-44e5-8f3a-0205b0920187",
   "metadata": {},
   "source": [
    "## Preprocessing all input text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554136b2-40b2-439f-8c46-cb2e4be08219",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [preprocessing(i) for i in X_train]\n",
    "X_val = [preprocessing(i) for i in X_val]\n",
    "X_test = [preprocessing(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248deef-e35c-4d50-bead-9524a397de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c6fcb-8d6a-4066-83c6-0b411c953872",
   "metadata": {},
   "source": [
    "## Tokenising input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44426730-f927-4106-8b0b-4390326578c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_padding(input_series, max_len, vocabulary=None, train=False):\n",
    "    \n",
    "    if type(input_series)!=list:\n",
    "        input_series = input_series.tolist()\n",
    "        \n",
    "    tokenized = [i.split() for i in input_series]\n",
    "    \n",
    "    if train:\n",
    "        # Building vocabulary\n",
    "        unique_words = ['<PAD>', '<UNK>'] + list(set(itertools.chain.from_iterable(tokenized)))\n",
    "        vocabulary = dict(zip(unique_words, range(len(unique_words))))\n",
    "        \n",
    "    assert vocabulary\n",
    "    \n",
    "    # Encoding and padding\n",
    "    document = []\n",
    "    \n",
    "    for i in tokenized:\n",
    "        tok_sent = [vocabulary[j] if j in vocabulary else 1 for j in i]\n",
    "        document.append(tok_sent)\n",
    "        \n",
    "    for i in range(len(document)):\n",
    "        if len(document[i])<=max_len:\n",
    "            document[i] = [0]*(max_len-len(document[i])) + document[i]\n",
    "        else:\n",
    "            document[i] = document[i][-max_len:]\n",
    "            \n",
    "    output = [np.array(i) for i in document]\n",
    "    \n",
    "    if train:\n",
    "        return np.vstack(output), vocabulary, len(vocabulary)\n",
    "    else:\n",
    "        return np.vstack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ffb95-0c9b-4135-867e-f86c0ae1c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for modeling\n",
    "max_length = seq_len = n_units = 150\n",
    "d_features = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e5723-ac24-4c58-858d-d166a29fdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded_doc, X_train_vocab, X_train_vocab_size = tokenizer_padding(X_train, max_len=max_length, train=True)\n",
    "X_val_padded_doc = tokenizer_padding(X_val, max_len=max_length, vocabulary=X_train_vocab)\n",
    "X_test_padded_doc = tokenizer_padding(X_test, max_len=max_length, vocabulary=X_train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5cee5-3891-42d6-a165-e6a6e5192259",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded_doc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdadd1d-666e-4320-8055-cb2ddd8b10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_padded_doc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36bae0-0f50-4ede-890b-062d62e0306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to tensors\n",
    "train_X = torch.tensor(X_train_padded_doc)\n",
    "train_y = torch.tensor(y_train_ohe.astype(float))\n",
    "\n",
    "test_X = torch.tensor(X_test_padded_doc)\n",
    "test_y = torch.tensor(y_test_ohe.astype(float))\n",
    "\n",
    "val_X = torch.tensor(X_val_padded_doc)\n",
    "val_y = torch.tensor(y_val_ohe.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e7598-9dbf-4125-afc0-ff423af880fd",
   "metadata": {},
   "source": [
    "## Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f702a50-5879-4023-b80d-1a6a04006ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# FOR TRAINING\n",
    "# Wrap tensors\n",
    "train_data = TensorDataset(train_X, train_y)\n",
    "\n",
    "# Sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# Dataloader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# FOR VALIDATING\n",
    "# Wrap tensors\n",
    "val_data = TensorDataset(val_X, val_y)\n",
    "\n",
    "# Sampler for sampling the data during validation for training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# Dataloader for val set\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b1cb6-c9f8-4b08-a0f7-dfd89b21d529",
   "metadata": {},
   "source": [
    "## Modeling without class balancing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44809e9d-8b45-4335-ba38-9728abce95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('..\\..\\..\\Pre-Trained Models\\word2vec\\GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ad58c-7abc-4243-bc4e-ce6cc609ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embedding matrix for pre-trained Word2Vec model\n",
    "embeddings_index = dict()\n",
    "\n",
    "# We will populate the embeddings_index dictionary with all the key<->vector pairs in the Word2Vec model\n",
    "for line in range(len(w2v_model.index_to_key)):\n",
    "    embeddings_index[w2v_model.index_to_key[line]] = w2v_model.get_vector(w2v_model.index_to_key[line])\n",
    "    \n",
    "# Create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((X_train_vocab_size, 300)) # Change X_train_vocab_size\n",
    "for word, i in X_train_vocab.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e12c2f-a438-4000-9d75-64aa93f574ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f340e3-7d7f-49bc-abda-131f19872c9d",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14458d9e-6d28-40e9-8164-e1c08647d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMClassifier(d_features=embedding_matrix.shape[1], embedding_matrix=embedding_matrix, vocab_size=X_train_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94bd0e-e825-42a7-aaf0-fad38af83853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df455a4-2aa1-4a11-976f-f62889ccb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755502c-5bb1-4324-847a-12e22a422474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define optimiser\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5996bc-6e69-4553-978b-254b880946a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e990af1-d90f-4924-8c76-d149bd14a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array(y_train.value_counts()[0]/y_train.value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90370690-f103-4397-b396-48a3a157f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b856e2-6e96-4881-9b9f-e4ebccb516a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of class weights to a tensor\n",
    "weights = torch.tensor(weight, dtype=torch.float)\n",
    "\n",
    "# Push weights to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# Define loss function\n",
    "cross_entropy = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "\n",
    "# No of training epochs\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f86bc-0865-405d-b731-c0bd97bfda4c",
   "metadata": {},
   "source": [
    "## Define Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335f967-58f1-4c7c-b6cb-6006a810d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # Empty list to save model predictions\n",
    "    total_preds = []\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update for every 50 batches\n",
    "        if step%50==0 and not step==0:\n",
    "            print ('Batch {:>5,} of {:>5,}.'.format(step, len(train_dataloader)))\n",
    "            \n",
    "        # Push batch to GPU\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id, labels = batch\n",
    "        \n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get model predictions for the current batch\n",
    "        preds = model(sent_id)\n",
    "        \n",
    "        # Compute loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        \n",
    "        # Add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to 1.0. It helps in preventing exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Model predictions are stored on GPU, so push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        \n",
    "        # Append model predictions\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "    # Compute training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # Predictions are in the form of (no. of batches, size of batch, no of classes)\n",
    "    # Reshape the prediction in form of (no of samples, no of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427105c-c0d4-453a-afb5-a216a290789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print ('\\nEvaluating...')\n",
    "    \n",
    "    # Deactivate dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # Empty list to save model predictions\n",
    "    total_preds = []\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        # Progress update for every 50 batches\n",
    "        if step%50==0 and not step==0:\n",
    "            print ('Batch {:>5,} of {:>5,}.'.format(step, len(val_dataloader)))\n",
    "            \n",
    "        # Push batch to GPU\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        \n",
    "        sent_id, labels = batch\n",
    "        \n",
    "        # Deactivate autograd()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Model predictions\n",
    "            preds = model(sent_id)\n",
    "            \n",
    "            # Compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            \n",
    "            total_loss = total_loss + loss.item()\n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            \n",
    "            total_preds.append(preds)\n",
    "            \n",
    "    # Compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    \n",
    "    # Reshape the predictions in form of (no of samples, no of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c32aba-e27d-4e1e-bf44-c5c84b631779",
   "metadata": {},
   "source": [
    "## Iterate through training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416d8ca-97f6-4ce9-865a-5fcd8af8c9d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    print ('\\nEpoch {:}/ {:}'.format(epoch+1, epochs))\n",
    "    \n",
    "    # Train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    # Evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    # Save the best model\n",
    "    if valid_loss<best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_model_weights/pytorch_bilstm.pt')\n",
    "        \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print (f\"\\nTraining Loss: {train_loss:.5f}\")\n",
    "    print (f\"Validation Loss: {valid_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b496953-263f-49c8-8d0a-9cfd8751b8a5",
   "metadata": {},
   "source": [
    "## Visualise training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1a685-bfe1-49ca-9d8f-30ffef744a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, 'g', valid_losses, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618a484-e3b9-4c6a-aad1-8ef9f2c0457a",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897c2ca-d76a-46f2-b38c-cc51344eca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99351955-13d5-4ee7-ab6a-f4fc1950d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_model_weights/pytorch_bilstm.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4756cd8-28b0-4ee1-b3da-8efc9c663e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model(test_X.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face7818-cc0a-4ba7-bb41-f0146402d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sigmoid function to outputs (sigmoid was auto applied by the loss function during training but the model architecture outputs predictions pre-sigmoid application)\n",
    "y_preds = nn.functional.sigmoid(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e183f-92ee-4646-9544-e35bf16cb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5146f06-e90c-4409-b025-7a07f7dde9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4463bb5-6817-4f5e-9147-0e1d1cf7ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change probabilities above to 1 for probabilities above 0.5\n",
    "y_hat = (y_preds>=0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebed47-68f7-4947-ad8c-a9ac2e0bc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test_ohe, y_hat, target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320a7e0-41ab-49ac-85e4-e6e921804d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test_ohe, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b13ea-9f91-4aee-be2b-49438543fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_ohe, y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
