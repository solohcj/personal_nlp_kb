{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba7cf0e-ec8e-4115-be32-e49aaf5adccc",
   "metadata": {},
   "source": [
    "## Level 4: Semantic Chunking <a id=\"SemanticChunking\"></a>\n",
    "Isn't it weird that we have a global constant for chunk size? Isn't it even weirder that our normal chunking mechanisms don't take into account the actual content?\n",
    "\n",
    "I'm not the only one who thinks so\n",
    "\n",
    "<!-- <div style=\"text-align: center;\">\n",
    "    <img src=\"static/SemanticChunkingtweet.png\" style=\"max-width:50%; height:auto;\"><br>\n",
    "    <span><i><a href=\"https://twitter.com/thesephist/status/1724159343237456248?s=46\">Source</a></i></span>\n",
    "</div> -->\n",
    "\n",
    "There has to be a better way - let's explore and find out.\n",
    "\n",
    "Embeddings represent the semantic meaning of a string. They don't do much on their own, but when compared to embeddings of other texts you can start to infer the relationship between chunks. I want to lean into this property and explore using embeddings to find clusters of semantically similar texts.\n",
    "\n",
    "The hypothesis is that semantically similar chunks should be held together.\n",
    "\n",
    "I tried a few methods:\n",
    "1) **Heirarchical clustering with positional reward** - I wanted to see how heirarchical clustering of sentence embeddings would do. But because I chose to split on sentences, there was an issue with small short sentences after a long one. You know? (like this last sentenence). They could change the meaning of a chunk, so I added a positional reward and clusters were more likely to form if they were sentences next to each other. This ended up being ok, but tuning the parameters was slow and unoptimal.\n",
    "2) **Find break points between sequential sentences** - Next up I tried a walk method. I started at the first sentence, got the embedding, then compared it to sentence #2, then compared #2 and #3 and so on. I was looking for \"break points\" where embedding distance was large. If it was above a threshold, then I considered it the start of a new semantic section. I originally tried taking embeddings of every sentence, but this turned out to be too noisy. So I ended up taking groups of 3 sentences (a window), then got an embedding, then dropped the first sentence, and added the next one. This worked out a bit better.\n",
    "\n",
    "I'll show method #2 here - It's not perfect by any means, but it's a good starting point for an exploration and I'd love to hear about how you think it could be improved.\n",
    "\n",
    "First, let's load up our essay that we'll run through. I'm just doing a single essay here to keep the tokens down.\n",
    "\n",
    "We'll be using Paul Graham's [MIT essay](https://paulgraham.com/mit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655fbb4-73a8-43dd-a6b7-565ecf85890a",
   "metadata": {},
   "source": [
    "Great, now that we have our sentences, I want to combine the sentence before and after so that we reduce noise and capture more of the relationships between sequential sentences.\n",
    "\n",
    "Let's create a function so we can use it again. The `buffer_size` is configurable so you can select how big of a window you want. Keep this number in mind for the later steps. I'll just use `buffer_size=1` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503f6ed2-399e-4684-85bc-aa08ac400c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\solom\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from SemChunk import Document\n",
    "from SemChunk_lexical_sentence_split import Document as Document_lexical\n",
    "# from SemChunk_backupv3 import Document as Document_backup\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845e7668-7e9e-4d62-a8fa-c108fcc676ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mit.txt') as file:\n",
    "    document = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ecb942-438e-465d-9530-130d63d2b94c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\solom\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('D:\\\\DSAI\\\\Pre-Trained Models\\\\all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('D:\\\\DSAI\\\\Pre-Trained Models\\\\all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932a2bb-c15d-4dbd-a664-600e92aa29c7",
   "metadata": {},
   "source": [
    "---\n",
    "Method with optimisation to calculation of cosine distances\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b348bcf3-2ff0-40f7-af4c-cbb1c7f8623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e16021-363e-49d8-af61-173b1ba8b14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__get_sentences_timing: 984.1868877410889ms\n",
      "__get_token_length_timing: 26.005029678344727ms\n",
      "__combine_sentences_timing: 0.9999275207519531ms\n",
      "__get_embeddings_timing: 5526.0608196258545ms\n",
      "__combine_sentence_embeddings_timing: 0.0ms\n",
      "__calculate_cosine_distances_timing: 6.002664566040039ms\n",
      "95 4362\n",
      "__get_optimal_chunks_timing: 7.999420166015625ms\n",
      "get_semantic_chunks_timing: 6.55625581741333s\n"
     ]
    }
   ],
   "source": [
    "chunks = doc.get_semantic_chunks(tokenizer=tokenizer, model=model, starting_threshold=95, step=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14b46e-aeaa-4240-bdd8-cdd97e15e8d2",
   "metadata": {},
   "source": [
    "---\n",
    "OG method with no optimisation to calculation of cosine distances\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100deaa1-de70-40d9-ae6c-7e622703de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_backup = Document_lexical(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3bf2ce-8c2f-45b4-8ca9-336facfc529f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__get_sentences_timing: 2.999544143676758ms\n",
      "__get_token_length_timing: 28.00464630126953ms\n",
      "__combine_sentences_timing: 0.9999275207519531ms\n",
      "__get_embeddings_timing: 5463.045120239258ms\n",
      "__combine_sentence_embeddings_timing: 0.0ms\n",
      "__calculate_cosine_distances_timing: 4.001855850219727ms\n",
      "95 4362\n",
      "__get_optimal_chunks_timing: 6.999492645263672ms\n",
      "get_semantic_chunks_timing: 5.510051012039185s\n"
     ]
    }
   ],
   "source": [
    "chunks_backup = doc_backup.get_semantic_chunks(tokenizer=tokenizer, model=model, starting_threshold=95, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815773c9-aebc-4e65-b787-c1abac18dcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1623962357009117"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6.4007/5.50647"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
