{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e421ede4-03c8-4333-b0ba-311d5b5b452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\solom\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b7b706-2de6-456b-984d-1ae66ebd0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:\\Data\\datasets\\imdb_long_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3979bca5-f021-41dd-9a16-3baf1d1d63fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>token_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
       "      <td>negative</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"The Cell\" is an exotic masterpiece, a dizzyin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'War movie' is a Hollywood genre that has been...</td>\n",
       "      <td>positive</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taut and organically gripping, Edward Dmytryk'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of the most significant quotes from the en...</td>\n",
       "      <td>positive</td>\n",
       "      <td>908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  token_lengths\n",
       "0  So im not a big fan of Boll's work but then ag...  negative            563\n",
       "1  \"The Cell\" is an exotic masterpiece, a dizzyin...  positive            749\n",
       "2  'War movie' is a Hollywood genre that has been...  positive            845\n",
       "3  Taut and organically gripping, Edward Dmytryk'...  positive            608\n",
       "4  One of the most significant quotes from the en...  positive            908"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fd28cd-5368-41f0-a46f-dde23d662f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], stratify=df['sentiment'], test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeafbc44-2efc-4ada-a19b-2d14471e7d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test\n",
      " sentiment\n",
      "positive    0.514089\n",
      "negative    0.485911\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "y_val\n",
      " sentiment\n",
      "positive    0.514605\n",
      "negative    0.485395\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "y_train\n",
      " sentiment\n",
      "positive    0.514071\n",
      "negative    0.485929\n",
      "Name: proportion, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"y_test\\n\", y_test.value_counts(normalize=True), '\\n')\n",
    "print (\"y_val\\n\", y_val.value_counts(normalize=True), '\\n')\n",
    "print (\"y_train\\n\", y_train.value_counts(normalize=True), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dde6d3-dc65-4df9-860b-1d20aca7fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading spacy as sentence chunker\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc1eb6e-c9e9-4216-abef-3c2797e71824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking each document\n",
    "# X_train_chunked = [nlp(each_sent) for each_sent in X_train]\n",
    "# X_train_chunked = [[sent for sent in nlp(each_sent).sents] for each_sent in X_train[:20]]\n",
    "\n",
    "\n",
    "# # Spacy takes too long, will chunk lexically first\n",
    "# X_train_chunked = [each_sent.split(\". \") for each_sent in X_train]\n",
    "# X_val_chunked = [each_sent.split(\". \") for each_sent in X_val\n",
    "# X_test_chunked = [each_sent.split(\". \") for each_sent in X_test]\n",
    "\n",
    "# Spacy takes too long, will chunk lexically first\n",
    "X_train_chunked = [each_sent.split(\". \") for each_sent in X_train[:20]]\n",
    "X_val_chunked = [each_sent.split(\". \") for each_sent in X_val[:20]]\n",
    "X_test_chunked = [each_sent.split(\". \") for each_sent in X_test[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1afd66c-d5ce-4a59-ae03-e2e2b7592a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max chunk length for X_train:  35\n",
      "Max chunk length for X_val:  67\n",
      "Max chunk length for X_test:  33\n"
     ]
    }
   ],
   "source": [
    "# Check max chunk length\n",
    "print (\"Max chunk length for X_train: \", max([len(chunks) for chunks in X_train_chunked]))\n",
    "print (\"Max chunk length for X_val: \", max([len(chunks) for chunks in X_val_chunked]))\n",
    "print (\"Max chunk length for X_test: \", max([len(chunks) for chunks in X_test_chunked]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14127fba-d0a3-427e-a26c-87dacf9d2c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4655"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a18e6b60-25a9-46c3-9a14-7521734121c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f722b56-3b31-416a-8084-f89e92383f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\solom\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder_path = \"D:\\\\DSAI\\\\Pre-Trained Models\\\\distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_path)\n",
    "# encoder = AutoModel.from_pretrained(encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a29c766-73ef-4aad-a86a-32bfd372d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = [[tokenizer(each_sent, padding='max_length', truncation=True, return_tensors='pt') for each_sent in each_doc] for each_doc in X_train_chunked]\n",
    "X_val_tokenized = [[tokenizer(each_sent, padding='max_length', truncation=True, return_tensors='pt') for each_sent in each_doc] for each_doc in X_val_chunked]\n",
    "X_test_tokenized = [[tokenizer(each_sent, padding='max_length', truncation=True, return_tensors='pt') for each_sent in each_doc] for each_doc in X_test_chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec02f1d6-9458-4add-b074-26702adee9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train_tokenized[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b3bba3-af81-46d5-914e-5d0094d407c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train_tokenized[0][0]['input_ids'].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de89c11-fabd-4b0d-9f04-4547598d6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_seq = [[sent['input_ids'].clone().detach() for sent in doc] for doc in X_train_tokenized]\n",
    "# train_mask = [[sent['attention_mask'].clone().detach() for sent in doc] for doc in X_train_tokenized]\n",
    "\n",
    "# val_seq = [[sent['input_ids'].clone().detach() for sent in doc] for doc in X_val_tokenized]\n",
    "# val_mask = [[sent['attention_mask'].clone().detach() for sent in doc] for doc in X_val_tokenized]\n",
    "\n",
    "# test_seq = [[sent['input_ids'].clone().detach() for sent in doc] for doc in X_test_tokenized]\n",
    "# test_mask = [[sent['attention_mask'].clone().detach() for sent in doc] for doc in X_test_tokenized]\n",
    "\n",
    "# train_label = torch.tensor(y_train.map({'positive':1, 'negative':0}).tolist())\n",
    "# val_label = torch.tensor(y_val.map({'positive':1, 'negative':0}).tolist())\n",
    "# test_label = torch.tensor(y_test.map({'positive':1, 'negative':0}).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd0f79ce-a785-438f-b12a-4dc5d563342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tensors(document_corpus, embedding_to_extract, max_chunks=30, max_sentence_token_len=512):\n",
    "    doc_list = []\n",
    "    for doc in document_corpus:\n",
    "        sent_list = []\n",
    "        for sent in doc:\n",
    "            sent_list.append(sent[embedding_to_extract].clone().detach()[0])\n",
    "            \n",
    "        sent_seqs = torch.stack(sent_list, dim=0)\n",
    "    \n",
    "        if sent_seqs.size()[0] < max_chunks: # keep it below 30 sentences for now\n",
    "            empty_sent_to_pad = torch.zeros(max_chunks-sent_seqs.size()[0], max_sentence_token_len)\n",
    "    \n",
    "            sent_seqs = torch.cat((empty_sent_to_pad, sent_seqs), dim=0)\n",
    "    \n",
    "        else:\n",
    "            sent_seqs = sent_seqs[:max_chunks, :]\n",
    "    \n",
    "        doc_list.append(sent_seqs)\n",
    "\n",
    "    return torch.stack(doc_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c298891a-8589-4829-b684-06c2048501b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = get_doc_tensors(X_train_tokenized, embedding_to_extract='input_ids')\n",
    "train_mask = get_doc_tensors(X_train_tokenized, embedding_to_extract='attention_mask')\n",
    "\n",
    "val_seq = get_doc_tensors(X_val_tokenized, embedding_to_extract='input_ids')\n",
    "val_mask = get_doc_tensors(X_val_tokenized, embedding_to_extract='attention_mask')\n",
    "\n",
    "test_seq = get_doc_tensors(X_test_tokenized, embedding_to_extract='input_ids')\n",
    "test_mask = get_doc_tensors(X_test_tokenized, embedding_to_extract='attention_mask')\n",
    "\n",
    "# train_label = torch.tensor(y_train.map({'positive':1, 'negative':0}).tolist())\n",
    "# val_label = torch.tensor(y_val.map({'positive':1, 'negative':0}).tolist())\n",
    "# test_label = torch.tensor(y_test.map({'positive':1, 'negative':0}).tolist())\n",
    "\n",
    "train_label = torch.tensor(y_train.map({'positive':1, 'negative':0}).tolist()[:20])\n",
    "val_label = torch.tensor(y_val.map({'positive':1, 'negative':0}).tolist()[:20])\n",
    "test_label = torch.tensor(y_test.map({'positive':1, 'negative':0}).tolist()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54d7d7e0-e72d-422a-b045-e24de3602769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_list = []\n",
    "# for doc in X_train_tokenized:\n",
    "#     sent_list = []\n",
    "#     for sent in doc:\n",
    "#         sent_list.append(sent['input_ids'].clone().detach()[0])\n",
    "        \n",
    "#     sent_seqs = torch.stack(sent_list, dim=0)\n",
    "\n",
    "#     if sent_seqs.size()[0] < 30: # keep it below 30 sentences for now\n",
    "#         empty_sent_to_pad = torch.zeros(30-sent_seqs.size()[0], 512)\n",
    "\n",
    "#         sent_seqs = torch.cat((empty_sent_to_pad, sent_seqs), dim=0)\n",
    "\n",
    "#     else:\n",
    "#         sent_seqs = sent_seqs[:30, :]\n",
    "\n",
    "#     doc_list.append(sent_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c988505-9961-4067-ae35-3643f0ddd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_seqs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "887b298e-9005-4e45-9b19-9a7eda41801c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.zeros(5, 512).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c15c73-8520-45af-8af6-bd368917026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_seqs[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bff2607-928f-40ce-939a-7e7c9418720a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7475533-e0b8-48a3-af27-2d42877f8cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462fafb4-4f87-437c-b329-ae96e75fe443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3763e0e-ec17-4172-9139-61fe2f3946a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_seq = torch.stack(doc_list, dim=0) # Need to pas to max length for this one! Else the shape wont fit\n",
    "\n",
    "# Probably have to do the torch.zeros method and slowly fill in the tensor??\n",
    "## Dont need can just manuall pad the fucking thing.. damn annoying - Solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "018c13ae-9d09-454c-aec8-5fb6143408da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "066ec74e-c2cf-4301-8480-57bb61b434df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18ceb388-e1a2-401b-bd2e-8c3b3ae504a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(X_train_tokenized[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57261f87-bba0-4ee8-9d16-d3ba41920542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_train_tokenized[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd9d09b7-fca2-4715-9ed6-bdfda7d77695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sent_seqs[-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "087c0cb0-2ac6-4ee6-b4a6-7ad48d40b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e2d70-e1f7-4329-9f06-d694011a83cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "342f1225-5dcc-493a-8825-18d8f0d775e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TRAINING\n",
    "# # Define batch size\n",
    "# batch_size = 8\n",
    "\n",
    "# # Wrap tensors\n",
    "# train_data = TensorDataset(doc_seq)\n",
    "# # Sampler for sampling the data during training\n",
    "# train_sampler = SequentialSampler(train_data)\n",
    "# # Dataloader for train set\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd215089-1149-4ab4-93b0-33e38ec7d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1997cec6-0410-4661-a9a8-b66d24dfc718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for num, batch in enumerate(train_dataloader):\n",
    "#     see = batch\n",
    "\n",
    "#     print (see[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2b34d56-b4ec-426b-b39e-37b9a7b33da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3236635-740e-48c4-b571-9daf47519332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sent_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fe82276-17d3-428d-a780-6580d9504d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.equal(see[0][-1], sent_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1835dfeb-daa5-4fa5-9845-0ffb0610bcf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# doc_seq[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6689195-4122-4193-a1c2-42cf07a29d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e202593-013b-460a-a852-cf6e744eaccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.equal(see[0][-2], doc_seq[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40cd79-162d-4477-9fcf-bf18537b982c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8e57e-e9dc-4370-9730-8e85a3d98d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92d8ca78-1c9f-49dd-9292-40de21b7b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.map({'positive':1, 'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db675143-4a25-4151-bfbe-a54b55189ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.tensor(y_train.map({'positive':1, 'negative':0}).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8eec708-ec0d-4e14-b36d-82dacd84aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TRAINING\n",
    "# Define batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_label)\n",
    "# Sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Dataloader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_label)\n",
    "# Sampler for sampling the data during validation for training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# Dataloader for val set\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Wrap tensors\n",
    "test_data = TensorDataset(test_seq, test_mask, test_label)\n",
    "# Sampler for sampling the data for testing\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "# Dataloader for test set\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d24b6-e199-4206-84af-4fd4e05b0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_anchor_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d53fa88-2399-45a4-a87f-4a75e958b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_anchor_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f82804-bc65-4675-b110-dd0c07c643f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_anchor_seq[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b5b37-5fc4-4ca7-908f-cee6a2dabeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785d149-9fe5-44c4-aea9-b508cc8d6f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train_chunked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac36f0-c83f-42e2-ac37-ab3fe9cfad44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(X_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c53926-4181-4a99-8717-92af7f282933",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1f188-5d36-4ba8-abcf-f42c5cf5360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b14626-0a3a-423f-abe4-2b9781c9a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define optimiser\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18460847-43b2-4ddb-99f8-c322878aed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = np.array(y_train.value_counts()[0]/y_train.value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0908ad-99da-4c99-8a77-0fd427644ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of class weights to a tensor\n",
    "weights = torch.tensor(weight, dtype=torch.float)\n",
    "\n",
    "# Push weights to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# Define loss function\n",
    "cross_entropy = nn.BCEWithLogitsLoss(pos_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559242b9-1630-4508-8dd4-22d5db6a778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # Empty list to save model predictions\n",
    "    total_preds = []\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update for every 50 batches\n",
    "        if step%50==0 and not step==0:\n",
    "            print ('Batch {:>5,} of {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # Push batch to GPU\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        anchor_id, anchor_mask, positive_id, positive_mask, negative_id, negative_mask = batch\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get model predictions for the current batch\n",
    "        anchor_output = model(anchor_id, anchor_mask)\n",
    "        positive_output = model(positive_id, positive_mask)\n",
    "        negative_output = model(negative_id, negative_mask)\n",
    "        \n",
    "        \"\"\"\n",
    "        nn.CosineSimilarity measures similarity between 2 outputs, the more similar, the bigger the score.\n",
    "        However for triplet loss, the positive cases are supposed to be closer and have a smaller score.\n",
    "        To make things easier, we flipped the negative and positive positions\n",
    "        i.e. loss(anchor, positive, negative) --> loss(anchor, negative, positive)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute loss \n",
    "        loss = loss_fn(mean_pooling(anchor_output, anchor_mask), mean_pooling(negative_output, negative_mask), mean_pooling(positive_output, positive_mask))\n",
    "\n",
    "        # Add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99269c15-840a-43b9-acc8-fd18b59ef47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print ('\\nEvaluating...')\n",
    "    \n",
    "    # Deactivate dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # Empty list to save model predictions\n",
    "    total_preds = []\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        # Progress update for every 50 batches\n",
    "        if step%50==0 and not step==0:\n",
    "            print ('Batch {:>5,} of {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # Push batch to GPU\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        anchor_id, anchor_mask, positive_id, positive_mask, negative_id, negative_mask = batch\n",
    "\n",
    "        # Deactivate autograd()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Get model predictions for the current batch\n",
    "            anchor_output = model(anchor_id, anchor_mask)\n",
    "            positive_output = model(positive_id, positive_mask)\n",
    "            negative_output = model(negative_id, negative_mask)\n",
    "            \n",
    "            \"\"\"\n",
    "            nn.CosineSimilarity measures similarity between 2 outputs, the more similar, the bigger the score.\n",
    "            However for triplet loss, the positive cases are supposed to be closer and have a smaller score.\n",
    "            To make things easier, we flipped the negative and positive positions\n",
    "            i.e. loss(anchor, positive, negative) --> loss(anchor, negative, positive)\n",
    "            \"\"\"\n",
    "\n",
    "            # Compute loss \n",
    "            loss = loss_fn(mean_pooling(anchor_output, anchor_mask), mean_pooling(negative_output, negative_mask), mean_pooling(positive_output, positive_mask))\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "    # Compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa283ff2-855a-48bf-acad-88f3e713cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    print ('\\nEpoch {:}/ {:}'.format(epoch+1, epochs))\n",
    "    \n",
    "    # Train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    # Evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    # Save the best model\n",
    "    if valid_loss<best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_model_weights/pytorch_bilstm.pt')\n",
    "        \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print (f\"\\nTraining Loss: {train_loss:.5f}\")\n",
    "    print (f\"Validation Loss: {valid_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191b225-66f0-47a3-a4c7-7345072d2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, 'g', valid_losses, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca2964-13de-452b-b0f2-74375bef51ae",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cef46a-f3df-4904-b3bf-391ff1aa44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'saved_model_weights/pytorch_bilstm.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
